---
title: "SVM Homework"
author: "Tao He"
date: "3/1/2022"
output: pdf_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE,out.width="0.9\\linewidth", fig.align  = 'center')
pacman::p_load(
ggplot2,
dplyr,
ISLR,
e1071
)
```

9.3
(a)
```{r}
X1 <- c(3, 2, 4, 1, 2, 4, 4)
X2 <- c(4, 2, 4, 4, 1, 3, 1)

Y <- c("red", "red", "red", "red", "blue", "blue", "blue")

plot(X1, X2, col = Y)
```

(b)
```{r}
plot(X1, X2, col = Y)
abline(-0.5, 1)
```

(c)
***sol.n:***
The classification rule is "Classify to "red" if X1 - X2 - 0.5 < 0, and classify to "blue" otherwise.

(d)
```{r}
plot(X1, X2, col = Y)
abline(-0.5, 1)
abline(-1, 1, lty = 2)
abline(0, 1, lty = 2)
```

The margin for the maximal margin hyperplane is 1/4.

(e)
***sol.n:***
The support vectors are the points (2,1), (2,2), (4,3) and (4,4).

(f)
***sol.n:***
If we move the seventh observation (4,1), the maximal margin hyperplane would not be changed as it is not a support vector.

(g)
```{r}
plot(X1, X2, col = Y)
abline(-0.2, 1)
```

The equation for this hyperplane is X1 - X2 -0.2 = 0


(h)
```{r}
plot(X1, X2, col = Y)
points(c(1.5), c(2.5), col = "blue")
```



9.5
(a)
```{r}
x1 <- runif(500) - 0.5
x2 <- runif(500) - 0.5
y <- 1 * (x1^2 - x2^2 > 0)
```

(b)
```{r}
df <- data.frame(x1= x1, x2= x2, y=as.factor(y))
plot(x1,x2,col = (2 - y))
```

(c)
```{r}
logit.fit <- glm(y ~ x1 + x2, family = "binomial")
summary(logit.fit)
```

(d)
```{r}
logit.probs <- predict(logit.fit, newdata = df, type = "response")
logit.preds <- rep(0, 500)
logit.preds[logit.probs > 0.5] <- 1

table(preds = logit.preds, truth = df$y)
```

```{r}
plot(x1, x2, col = (2-logit.preds))
```

The decision boundary is linear.


(e)
```{r warning=FALSE}
logit.fit1 <- glm(y ~ poly(x1, 2, raw = T) + poly(x2, 2, raw = T), family = "binomial")
summary(logit.fit1)
```

None of the variables are statistically significant.

(f)
```{r}
logit.fit1.probs <- predict(logit.fit1, newdata = df, type = "response")
logit.fit1.preds <- rep(0, 500)
logit.fit1.preds[logit.fit1.probs > 0.5] <- 1

table(preds = logit.fit1.preds, truth = df$y)
```

```{r}
plot(x1, x2, col = (2- logit.fit1.preds))
```

The decision boundary should be obviously non-linear.
Quadratic transformations of x1 and x2 result in perfect separation.


(g)
```{r}
# best model
# linear
tune.out <- tune(svm, y ~ x1 + x2, data = df, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
bestmod <- tune.out$best.model

pred <- predict(bestmod, newdata = df, type = "response")
table(predict = pred, truth = df$y)
plot(x1, x2, col = pred)
```


(h)
```{r}
# non-linear
tune.out <- tune(svm, y ~ x1 + x2, data = df, kernel = "radial", ranges = list(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4)))
bestmod <- tune.out$best.model

pred <- predict(bestmod, newdata = df, type = "response")
table(predict = pred, truth = df$y)
plot(x1, x2, col = pred)
```


(i)
***sol.n:***
We may conclude that both SVM with non-linear kernel and logistic regression are useful for finding non-linear decision boundaries. 

On the other words, SVM with linear kernel and logistic regression without any interaction term perform badly during finding non-linear decision boundaries.



9.7
(a)
```{r}
data <- ifelse(Auto$mpg > median(Auto$mpg), 1, 0) 
Auto$mpglevel <- as.factor(data)
```

(b)
```{r}
set.seed(679)
tune.out <- tune(svm, mpglevel ~ ., data = Auto, kernel = "linear", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000)))
bestmod <- tune.out$best.model
bestmod
```
A cost of 1 perform best.

(c)
```{r}
# radial
set.seed(679)
tune.out <- tune(svm, mpglevel ~ ., data = Auto, kernel = "radial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), gamma = c(0.01, 0.1, 1, 5, 10, 100)))
tune.out$best.parameters

# polynomial
tune.out <- tune(svm, mpglevel ~ ., data = Auto, kernel = "polynomial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), degree = c(2, 3, 4)))
tune.out$best.parameters
```

For a radial kernel, the lowest cross-validation error is obtained for a gamma of 0.01 and a cost of 100.

For a polynomial kernel, the lowest cross-validation error is obtained for a degree of 2 and a cost of 100.

(d)
```{r}

```




9.8
(a)
```{r}
set.seed(679)
train <- sample(nrow(OJ), 800)
OJ.train <- OJ[train,]
OJ.test <- OJ[-train,]
```

(b)
```{r}
svm.linear <- svm(Purchase ~ ., data = OJ, subset = train, kernel = "linear", cost = 0.01)
summary(svm.linear)
```

Support vector classifier creates 427 support vectors out of 800 training points. Out of these, 213 belong to level CH and remaining 214 belong to level MM.

(c)
```{r}
# training error
pred <- predict(svm.linear, newdata = OJ.train, type = "response")
result <- table(OJ.train$Purchase, pred)
result

training.error.rate <- (result[2,1] + result[1,2])/(result[1,1] + result[1,2] + result[2,1] + result[2,2])
training.error.rate
```

The training error rate os 0.15

```{r}
# test error
pred <- predict(svm.linear, newdata = OJ.test, type = "response")
result <- table(OJ.test$Purchase, pred)
result

test.error.rate <- (result[2,1] + result[1,2])/(result[1,1] + result[1,2] + result[2,1] + result[2,2])
test.error.rate
```

The test error rate is 0.2.

(d)
```{r}
set.seed(679)
tune.out <- tune(svm, Purchase ~ ., data = OJ.train, kernel = "linear", ranges = list(cost = 10^seq(-2, 1, by = 0.25)))
bestmod <- tune.out$best.parameters
bestmod
```
The best cost is 0.01.

(e)
```{r}
svm.linear <- svm(Purchase ~ ., kernel = "linear", data = OJ.train, cost = tune.out$best.parameter$cost)
train.pred <- predict(svm.linear, newdata = OJ.train, type = "response")
result <- table(OJ.train$Purchase, train.pred)
result

training.error.rate <- (result[2,1] + result[1,2])/(result[1,1] + result[1,2] + result[2,1] + result[2,2])
training.error.rate
```

```{r}
test.pred <- predict(svm.linear, newdata = OJ.test, type = "response")
result <- table(OJ.test$Purchase, test.pred)

result

test.error.rate <- (result[2,1] + result[1,2])/(result[1,1] + result[1,2] + result[2,1] + result[2,2])
test.error.rate
```

(f)
```{r}
svm.radial <- svm(Purchase ~ ., kernel = "radial", data = OJ.train)
summary(svm.radial)
```

```{r}
train.pred <- predict(svm.radial, newdata = OJ.train, type = "response")
result <- table(OJ.train$Purchase, train.pred)
result

training.error.rate <- (result[2,1] + result[1,2])/(result[1,1] + result[1,2] + result[2,1] + result[2,2])
training.error.rate
```

```{r}
test.pred <- predict(svm.radial, newdata = OJ.test, type = "response")
result <- table(OJ.test$Purchase, test.pred)

result

test.error.rate <- (result[2,1] + result[1,2])/(result[1,1] + result[1,2] + result[2,1] + result[2,2])
test.error.rate
```

```{r}
set.seed(679)
tune.out <- tune(svm, Purchase ~ ., data = OJ.train, kernel = "radial", ranges = list(cost = 10^seq(-2, 1, by = 0.25)))
summary(tune.out)
```

```{r}
svm.radial <- svm(Purchase ~ ., kernel = "radial", data = OJ.train, cost = tune.out$best.parameter$cost)
summary(svm.radial)
```
```{r}
train.pred <- predict(svm.radial, newdata = OJ.train, type = "response")
result <- table(OJ.train$Purchase, train.pred)
result

training.error.rate <- (result[2,1] + result[1,2])/(result[1,1] + result[1,2] + result[2,1] + result[2,2])
training.error.rate
```

```{r}
test.pred <- predict(svm.radial, newdata = OJ.test, type = "response")
result <- table(OJ.test$Purchase, test.pred)

result

test.error.rate <- (result[2,1] + result[1,2])/(result[1,1] + result[1,2] + result[2,1] + result[2,2])
test.error.rate
```

(g)
```{r}
svm.poly <- svm(Purchase ~ ., kernel = "polynomial", data = OJ.train, degree = 2)
summary(svm.poly)
```

```{r}
train.pred <- predict(svm.poly, newdata = OJ.train, type = "response")
result <- table(OJ.train$Purchase, train.pred)
result

training.error.rate <- (result[2,1] + result[1,2])/(result[1,1] + result[1,2] + result[2,1] + result[2,2])
training.error.rate
```

```{r}
test.pred <- predict(svm.radial, newdata = OJ.test, type = "response")
result <- table(OJ.test$Purchase, test.pred)

result

test.error.rate <- (result[2,1] + result[1,2])/(result[1,1] + result[1,2] + result[2,1] + result[2,2])
test.error.rate
```

```{r}
set.seed(679)
tune.out <- tune(svm, Purchase ~ ., data = OJ.train, kernel = "polynomial", degree = 2, ranges = list(cost = 10^seq(-2, 1, by = 0.25)))
summary(tune.out)
```

```{r}
svm.poly <- svm(Purchase ~ ., kernel = "polynomial", degree = 2, data = OJ.train, cost = tune.out$best.parameter$cost)
summary(svm.poly)
```

```{r}
train.pred <- predict(svm.poly, newdata = OJ.train, type = "response")
result <- table(OJ.train$Purchase, train.pred)
result

training.error.rate <- (result[2,1] + result[1,2])/(result[1,1] + result[1,2] + result[2,1] + result[2,2])
training.error.rate
```

```{r}
test.pred <- predict(svm.radial, newdata = OJ.test, type = "response")
result <- table(OJ.test$Purchase, test.pred)

result

test.error.rate <- (result[2,1] + result[1,2])/(result[1,1] + result[1,2] + result[2,1] + result[2,2])
test.error.rate
```


(h)
***sol.n:***
In conclusion, radial kernel could produce minimum misclassification error on both train and test data.

